# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2001-2024, Python Software Foundation
# This file is distributed under the same license as the Python package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# Dmytro Kazanzhy, 2023
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Python 3.13\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-11-19 01:00+0000\n"
"PO-Revision-Date: 2021-06-28 01:17+0000\n"
"Last-Translator: Dmytro Kazanzhy, 2023\n"
"Language-Team: Ukrainian (https://app.transifex.com/python-doc/teams/5390/uk/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: uk\n"
"Plural-Forms: nplurals=4; plural=(n % 1 == 0 && n % 10 == 1 && n % 100 != 11 ? 0 : n % 1 == 0 && n % 10 >= 2 && n % 10 <= 4 && (n % 100 < 12 || n % 100 > 14) ? 1 : n % 1 == 0 && (n % 10 ==0 || (n % 10 >=5 && n % 10 <=9) || (n % 100 >=11 && n % 100 <=14 )) ? 2: 3);\n"

#: ../../library/urllib.robotparser.rst:2
msgid ":mod:`!urllib.robotparser` ---  Parser for robots.txt"
msgstr ""

#: ../../library/urllib.robotparser.rst:10
msgid "**Source code:** :source:`Lib/urllib/robotparser.py`"
msgstr "**Вихідний код:** :source:`Lib/urllib/robotparser.py`"

#: ../../library/urllib.robotparser.rst:20
msgid ""
"This module provides a single class, :class:`RobotFileParser`, which answers"
" questions about whether or not a particular user agent can fetch a URL on "
"the web site that published the :file:`robots.txt` file.  For more details "
"on the structure of :file:`robots.txt` files, see "
"http://www.robotstxt.org/orig.html."
msgstr ""
"Цей модуль надає єдиний клас, :class:`RobotFileParser`, який відповідає на "
"питання про те, чи може певний агент користувача отримати URL-адресу на веб-"
"сайті, який опублікував файл :file:`robots.txt`. Щоб отримати докладнішу "
"інформацію про структуру файлів :file:`robots.txt`, див. "
"http://www.robotstxt.org/orig.html."

#: ../../library/urllib.robotparser.rst:28
msgid ""
"This class provides methods to read, parse and answer questions about the "
":file:`robots.txt` file at *url*."
msgstr ""
"Цей клас надає методи читання, аналізу та відповідей на запитання щодо файлу"
" :file:`robots.txt` за адресою *url*."

#: ../../library/urllib.robotparser.rst:33
msgid "Sets the URL referring to a :file:`robots.txt` file."
msgstr "Встановлює URL-адресу, яка посилається на файл :file:`robots.txt`."

#: ../../library/urllib.robotparser.rst:37
msgid "Reads the :file:`robots.txt` URL and feeds it to the parser."
msgstr ""
"Читає URL-адресу :file:`robots.txt` і передає її синтаксичному аналізатору."

#: ../../library/urllib.robotparser.rst:41
msgid "Parses the lines argument."
msgstr "Розбирає аргумент рядків."

#: ../../library/urllib.robotparser.rst:45
msgid ""
"Returns ``True`` if the *useragent* is allowed to fetch the *url* according "
"to the rules contained in the parsed :file:`robots.txt` file."
msgstr ""
"Повертає ``True``, якщо *useragent* дозволено отримувати *url* згідно з "
"правилами, що містяться в проаналізованому файлі :file:`robots.txt`."

#: ../../library/urllib.robotparser.rst:51
msgid ""
"Returns the time the ``robots.txt`` file was last fetched.  This is useful "
"for long-running web spiders that need to check for new ``robots.txt`` files"
" periodically."
msgstr ""
"Повертає час останнього отримання файлу ``robots.txt``. Це корисно для "
"тривалих веб-павуків, яким потрібно періодично перевіряти наявність нових "
"файлів ``robots.txt``."

#: ../../library/urllib.robotparser.rst:57
msgid ""
"Sets the time the ``robots.txt`` file was last fetched to the current time."
msgstr ""
"Встановлює час останнього отримання файлу ``robots.txt`` на поточний час."

#: ../../library/urllib.robotparser.rst:62
msgid ""
"Returns the value of the ``Crawl-delay`` parameter from ``robots.txt`` for "
"the *useragent* in question.  If there is no such parameter or it doesn't "
"apply to the *useragent* specified or the ``robots.txt`` entry for this "
"parameter has invalid syntax, return ``None``."
msgstr ""
"Повертає значення параметра ``Crawl-delay`` з ``robots.txt`` для "
"відповідного *useragent*. Якщо такого параметра немає або він не "
"застосовується до вказаного *useragent* або запис ``robots.txt`` для цього "
"параметра має недійсний синтаксис, поверніть ``None``."

#: ../../library/urllib.robotparser.rst:71
msgid ""
"Returns the contents of the ``Request-rate`` parameter from ``robots.txt`` "
"as a :term:`named tuple` ``RequestRate(requests, seconds)``. If there is no "
"such parameter or it doesn't apply to the *useragent* specified or the "
"``robots.txt`` entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"Повертає вміст параметра ``Request-rate`` з ``robots.txt`` як :term:`named "
"tuple` ``RequestRate(requests, seconds)``. Якщо такого параметра немає або "
"він не застосовується до вказаного *useragent* або запис ``robots.txt`` для "
"цього параметра має недійсний синтаксис, поверніть ``None``."

#: ../../library/urllib.robotparser.rst:81
msgid ""
"Returns the contents of the ``Sitemap`` parameter from ``robots.txt`` in the"
" form of a :func:`list`. If there is no such parameter or the ``robots.txt``"
" entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"Повертає вміст параметра ``Карта сайту`` з ``robots.txt`` у формі "
":func:`list`. Якщо такого параметра немає або запис ``robots.txt`` для цього"
" параметра має недійсний синтаксис, поверніть ``None``."

#: ../../library/urllib.robotparser.rst:89
msgid ""
"The following example demonstrates basic use of the :class:`RobotFileParser`"
" class::"
msgstr ""
"Наступний приклад демонструє базове використання класу "
":class:`RobotFileParser`::"

#: ../../library/urllib.robotparser.rst:92
msgid ""
">>> import urllib.robotparser\n"
">>> rp = urllib.robotparser.RobotFileParser()\n"
">>> rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n"
">>> rp.read()\n"
">>> rrate = rp.request_rate(\"*\")\n"
">>> rrate.requests\n"
"3\n"
">>> rrate.seconds\n"
"20\n"
">>> rp.crawl_delay(\"*\")\n"
"6\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\")\n"
"False\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")\n"
"True"
msgstr ""

#: ../../library/urllib.robotparser.rst:12
msgid "WWW"
msgstr ""

#: ../../library/urllib.robotparser.rst:12
msgid "World Wide Web"
msgstr ""

#: ../../library/urllib.robotparser.rst:12
msgid "URL"
msgstr "URL"

#: ../../library/urllib.robotparser.rst:12
msgid "robots.txt"
msgstr ""
